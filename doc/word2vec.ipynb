{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    342\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: libcublas.so.8.0: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db5ae1630ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munidecode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/angeldelgado/anaconda3/lib/python3.6/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TEXT MINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/cuatro_corazones_con_freno_y_marcha_atras', 'r') as file_obj: #encoding=\"ISO-8859-1\"\n",
    "    text = file_obj.read()\n",
    "\n",
    "with open('data/caperucita_roja', 'r') as file_obj: #encoding=\"ISO-8859-1\"\n",
    "    text = file_obj.read()\n",
    "    \n",
    "with open('data/stop_words', 'r') as file_obj:\n",
    "    stopwords = file_obj.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class data_preparation(object):\n",
    "       \n",
    "    \n",
    "    \n",
    "    def make_disintegration(self, text):\n",
    "        \n",
    "        '''\n",
    "        the main object is to convert a text to a \"plain text\" with only lower letters and stops.\n",
    "        \n",
    "        input :  real text\n",
    "        output : plain text\n",
    "        '''\n",
    "        \n",
    "        text = re.sub(r'\\n+','\\n', text)\n",
    "        text = re.sub(r'<.*?>',' ', text)\n",
    "        text = re.sub('^\\\\[a-zA-Z]*',' ', text)\n",
    "        \n",
    "        text = re.sub(r',|;|\\n|—|-|“|”|:|\\\"','.', text)\n",
    "        text = re.sub(r'\\?|¿|!|¡','.', text)\n",
    "        text = re.sub(r'\\)|\\(','.',text)\n",
    "        text = re.sub(r' \\.','.', text)\n",
    "        \n",
    "        text = re.sub(r'\\.+','. ', text)\n",
    "        text = re.sub(' +|\\t',' ', text)\n",
    "        \n",
    "        return text.lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_sentences(self, text):\n",
    "        \n",
    "        '''\n",
    "        text: plain text with only lower letters and stops.\n",
    "        \n",
    "        setences: list of text chunks split by stops.\n",
    "        '''\n",
    "        \n",
    "        sentences = []\n",
    "\n",
    "        for sentence in text.split('.'):\n",
    "            sentences.append(sentence.split())\n",
    "            \n",
    "        return sentences\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_dictionary(self, text, stopwords,vocab_size):\n",
    "        \n",
    "        '''\n",
    "        This is made for getting an index-representation for the words in the text.\n",
    "        It only creates an index for the \"vocab_size\" most popular words in the text.\n",
    "        \n",
    "        text: plain text with only lower letters and stops.\n",
    "        \n",
    "        dicc_w2i: mapping word\n",
    "        '''  \n",
    "        \n",
    "        words = []\n",
    "        \n",
    "        \n",
    "        for word in text.split(' '): \n",
    "            \n",
    "            word = re.sub(r'\\.','',word) #con esto quitamos el punto de la última palabra en cada frase\n",
    "\n",
    "            if ((word not in stopwords) and (re.match('^[a-zA-Z]*$',unidecode(word))) and (word != '')):\n",
    "                \n",
    "                words.append(word)\n",
    "              \n",
    "            \n",
    "        count = collections.Counter(words).most_common(vocab_size-1) # el -1 es porque para guardar dentro del vocabulario un espacio para las palabras desconocidas\n",
    "    \n",
    "    \n",
    "        \n",
    "        dicc_w2i = dict([(counter[0], index+1) for (index, counter) in enumerate(count)]) # el index+1 es para reservar el índice 0 para las palabras desconocidas\n",
    "        dicc_i2w = dict([(index+1, counter[0]) for (index, counter) in enumerate(count)])\n",
    "        \n",
    "        dicc = {'w2i' : dicc_w2i, 'i2w' : dicc_i2w}\n",
    "        \n",
    "        \n",
    "        with open(\"model/dicc.pkl\",\"wb\") as file:\n",
    "            pickle.dump(dicc,file)\n",
    "            \n",
    "            \n",
    "        return (dicc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' \n",
    "    def get_word_word(self, sentences, stopwords, window_size = 2):\n",
    "    \n",
    "        data = []\n",
    "    \n",
    "        for sentence in sentences:\n",
    "            sentence = [word for word in sentence if ((word.lower() not in stopwords) and (re.match('^[a-zA-Z]*$',unidecode(word))))]\n",
    "    \n",
    "     \n",
    "            for word_index, word in enumerate(sentence):      \n",
    "                neighbourhood_words = sentence[max(word_index - window_size, 0) : min(word_index + window_size, len(sentence)) + 1]\n",
    "            \n",
    "        \n",
    "                for neighbour_word in neighbourhood_words:       \n",
    "                    neighbour_word = neighbour_word.lower()\n",
    "                    word = word.lower()      \n",
    "            \n",
    "            \n",
    "                    if neighbour_word != word:\n",
    "                        data.append([word, neighbour_word])\n",
    "                                      \n",
    "        return(data)\n",
    "    '''\n",
    "\n",
    "    def get_word_list(self, sentences, stopwords, window_size = 2):\n",
    "        \n",
    "        '''\n",
    "        Given a list of sentences, it makes a list with each word and the \"window_size\" words around.\n",
    "        \n",
    "        sentence = ['word1 word2 word3...', '...', ...]\n",
    "        data =  = [word2, [word1,word2]]\n",
    "        '''\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for sentence in sentences:   \n",
    "            sentence = [word for word in sentence if ((word.lower() not in stopwords) and (re.match('^[a-zA-Z]*$',unidecode(word))))]\n",
    "       \n",
    "    \n",
    "            for word_index, word in enumerate(sentence):\n",
    "                word = word.lower()\n",
    "                neighbourhood_words = sentence[max(word_index - window_size, 0) : min(word_index + window_size, len(sentence)) + 1]\n",
    "                neighbourhood_words = [neighbour.lower() for neighbour in neighbourhood_words if neighbour.lower()!=word]\n",
    "       \n",
    "                \n",
    "                while (len(neighbourhood_words)<(2*window_size)):\n",
    "                    neighbourhood_words.append(word)\n",
    "                \n",
    "                                               \n",
    "                data.append([word, neighbourhood_words])    \n",
    "                    \n",
    "        return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class word2vec(object):\n",
    "    \n",
    "    '''\n",
    "    Object for implementing word2vec algorithm in a dataset with the requiered structure.\n",
    "    \n",
    "    Requires:\n",
    "    \n",
    "        - The dataset\n",
    "        - Dictionary of words and indexes\n",
    "        - Parameters\n",
    "        \n",
    "    Saves in local:\n",
    "    \n",
    "        - Tensorflow graph\n",
    "        - Tensors W1 and b1 as a np.array for the encoder and decoder.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim):\n",
    "        \n",
    "        '''\n",
    "        Feed forward neuralnet architecture with two hidden layers.\n",
    "        \n",
    "        input: word vector in one-hot-encoding representation\n",
    "        label: window size words\n",
    "        \n",
    "        The vector representation of the word is the tensor \"encoder\"\n",
    "        '''\n",
    "        \n",
    "        # DIMENSIONS\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.optimizer_step = 0.01\n",
    "        \n",
    "    \n",
    "        # NEURALNET\n",
    "        self.input_data = tf.placeholder(tf.float32, shape=(None, vocab_size), name = 'input_data')\n",
    "        self.output_data = tf.placeholder(tf.float32, shape=(None, vocab_size), name = 'output_data')\n",
    "\n",
    "        \n",
    "        self.W1 = tf.Variable(tf.random_normal([vocab_size, embedding_dim]), name = 'W1')\n",
    "        self.b1 = tf.Variable(tf.random_normal([embedding_dim]), name = 'b1')\n",
    "        self.vector = tf.add(tf.matmul(self.input_data,self.W1), self.b1, name = 'encoder')\n",
    "        \n",
    "        \n",
    "        self.W2 = tf.Variable(tf.random_normal([embedding_dim, vocab_size]), name = 'W2')\n",
    "        self.b2 = tf.Variable(tf.random_normal([vocab_size]), name = 'b2')\n",
    "        self.prediction = tf.nn.softmax(tf.add( tf.matmul(self.vector, self.W2), self.b2), name = 'prediction')\n",
    "\n",
    "        \n",
    "        \n",
    "        # OPTIMIZATION\n",
    "        self.cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(self.output_data * tf.log(self.prediction), reduction_indices=[1]))\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(self.optimizer_step).minimize(self.cross_entropy_loss)   \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def to_one_hot(self, data_point_index):\n",
    "        \n",
    "        temp = np.zeros(self.vocab_size)\n",
    "        temp[data_point_index] = 1\n",
    "        return temp\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def training_data(self, data):\n",
    "        \n",
    "        '''\n",
    "        First it transforms the word data to the index representation.\n",
    "        Then it transforms the index representation to one-hot-encoding representation.\n",
    "        \n",
    "        It works with training data structure ([[word, [word,word]],...]) and with predictive ([[word],...])\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        with open('./model/dicc.pkl','rb') as file:\n",
    "            dicc = pickle.load(file)\n",
    "            dicc_w2i = dicc['w2i']\n",
    "\n",
    "            \n",
    "            \n",
    "        input_train = []\n",
    "        output_train = []\n",
    "        \n",
    "        \n",
    "        #if(len(data[0])==2): #if data is predictive data\n",
    "        for data_word in data:\n",
    "            \n",
    "            #input_indexes = word2int[data_word[0]]            \n",
    "            input_index = dicc_w2i[data_word[0]] if  data_word[0] in dicc_w2i.keys() else 0 #el imput siempre es solo una palabra\n",
    "            input_train.append(self.to_one_hot(input_index))\n",
    "            \n",
    "            output_index = []\n",
    "            for word in np.array(data_word[1]).reshape(-1):#el output es más enrevesado porque puede ser una palabra o una lista de palabras\n",
    "                output_index.append(dicc_w2i[word] if word in dicc_w2i.keys() else 0)\n",
    "                \n",
    "            #output_index = [dicc_w2i[word] for word in np.array(data_word[1]).reshape(-1)] #el output es más enrevesado porque puede ser una palabra o una lista de palabras\n",
    "            output_train.append(self.to_one_hot(output_index))\n",
    "\n",
    "        \n",
    "        '''elif(len(data[0])==1): #if data is training data\n",
    "            \n",
    "            for data_word in data:\n",
    "            \n",
    "                #input_indexes = word2int[data_word[0]]\n",
    "                input_indexes = word2int[np.reshape(data_word,(1,-1))[0]] #el imput siempre es solo una palabra\n",
    "                input_train.append(self.to_one_hot(input_indexes, self.vocab_size))  \n",
    "              \n",
    "        \n",
    "        else: \n",
    "            continue\n",
    "        '''\n",
    "        \n",
    "        input_train = np.asarray(input_train)\n",
    "        output_train = np.asarray(output_train)  \n",
    "        \n",
    "        \n",
    "        return (input_train,output_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, x_train, y_train, batch_size = 256):\n",
    "        \n",
    "        '''\n",
    "        Train the tensorflow graph.\n",
    "        '''\n",
    "        \n",
    "        n_data = len(x_train)\n",
    "        n_batch = n_data//batch_size\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            for batch_index in range(n_batch):\n",
    "                \n",
    "                \n",
    "                \n",
    "                x = x_train[(n_batch*batch_size):((n_batch+1)*batch_size)]\n",
    "                y = y_train[(n_batch*batch_size):((n_batch+1)*batch_size)]\n",
    "\n",
    "                    \n",
    "                sess.run([self.train_step,self.vector], feed_dict={self.input_data: x, self.output_data: y})\n",
    "                \n",
    "                #if (batch_index+1)%(n_batch//100)==0:\n",
    "                #    print('Progress: ', batch_index//n_batch*100)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, \"./model/model\")\n",
    "            \n",
    "            W1 = sess.run(self.W1)\n",
    "            b1 = sess.run(self.b1)\n",
    "            \n",
    "            np.save('model/W1.npy', W1)\n",
    "            np.save('model/b1.npy', b1)\n",
    "        \n",
    "        \n",
    "        return (W1, b1)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    def encoder(self, words):\n",
    "        \n",
    "        '''\n",
    "        Load the save graph and execute for the words in one-hot-representation\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            saver = tf.train.import_meta_graph('./model/model.meta')\n",
    "            saver.restore(sess,tf.train.latest_checkpoint('./model'))\n",
    "\n",
    "            graph = tf.get_default_graph()\n",
    "            \n",
    "            input_data = graph.get_tensor_by_name(\"input_data:0\")\n",
    "            output_data = graph.get_tensor_by_name(\"output_data:0\")\n",
    "            vector = graph.get_tensor_by_name(\"vector:0\")\n",
    "                \n",
    "            x_train = self.prediction_data(word)    \n",
    "            vector = sess.run(vector, feed_dict={input_data: x_train})\n",
    "        '''\n",
    "        \n",
    "        W1 = np.load('model/W1.npy')\n",
    "        b1 = np.load('model/b1.npy')\n",
    "        \n",
    "        with open('./model/dicc.pkl','rb') as file:\n",
    "            dicc = pickle.load(file)\n",
    "        \n",
    "        dicc_w2i = dicc['w2i']\n",
    "        \n",
    "        indexes = [dicc_w2i[word] if word in dicc_w2i else 0 for word in words]\n",
    "        input_data = [self.to_one_hot(index) for index in indexes]\n",
    "        input_data = np.reshape(input_data,(-1,self.vocab_size))\n",
    "        vectors = np.dot(input_data,W1)+b1\n",
    "        \n",
    "        return vectors.tolist()     \n",
    "           \n",
    "    \n",
    "    \n",
    "    def decoder(self, vectors):\n",
    "        \n",
    "        '''\n",
    "        Returns the nearest word in the word-representation for the given vectors.\n",
    "        \n",
    "        It loads the graph, extract the tensors W1 y b1 and \n",
    "        '''\n",
    "        \n",
    "        W1 = np.load('model/W1.npy')\n",
    "        b1 = np.load('model/b1.npy')\n",
    "        \n",
    "        with open('./model/dicc.pkl','rb') as file:\n",
    "            dicc = pickle.load(file)\n",
    "        \n",
    "        dicc_i2w = dicc['i2w']\n",
    "    \n",
    "        def euclidean_dist(vector1, vector2): return np.sqrt(np.sum((vector1-vector2)**2))   \n",
    "\n",
    "        \n",
    "        '''with tf.Session() as sess:\n",
    "\n",
    "            saver = tf.train.import_meta_graph('./model/model.meta')\n",
    "            saver.restore(sess,tf.train.latest_checkpoint('./model'))\n",
    "   \n",
    "            graph = tf.get_default_graph()\n",
    "            input_data = graph.get_tensor_by_name(\"input_data:0\")\n",
    "            output_data = graph.get_tensor_by_name(\"output_data:0\")\n",
    "            vocab_vector = graph.get_tensor_by_name(\"vector:0\")        \n",
    "        \n",
    "        '''\n",
    "        vocab_vectors = W1+b1\n",
    "        \n",
    "\n",
    "        words = []\n",
    "        \n",
    "        for vector in vectors:\n",
    "            \n",
    "            distances = np.apply_along_axis(euclidean_dist, 1, vocab_vectors, vector)\n",
    "            nearest_index = np.argmin(distances)\n",
    "            nearest_word = dicc_i2w[nearest_index] if nearest_index!=0 else ''\n",
    "            \n",
    "            words.append(nearest_word)\n",
    "            \n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 200\n",
    "embedding_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "prepare = data_preparation()\n",
    "\n",
    "text = prepare.make_disintegration(text)\n",
    "sent = prepare.get_sentences(text)\n",
    "dicc = prepare.get_dictionary(text, stopwords, vocab_size)\n",
    "data = prepare.get_word_list(sent, stopwords,window_size =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec(vocab_size, embedding_dim)\n",
    "x_train,y_train = model.training_data(data)\n",
    "_ = model.train(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caperucita', 'lobo', 'abuela']\n"
     ]
    }
   ],
   "source": [
    "vectors = model.encoder(['caperucita','lobo','abuela'])\n",
    "palabras = model.decoder(vectors)\n",
    "print(palabras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
